# Objective: 
Detection of a electrical panel door using a Intel Realsense camera and Opencv libraries. 

Project is focused towards incorporating this technology into a drone for assisting it to open the door in air autonomously. 
Drones are being used extensively for surveillance and reconnaissance tasks.
But in the recent past they are also being employed for interacting physically with the environment.
This can involve tasks like picking up packages or boxes, mounting some sensor on a wall or opening doors while hovering.
Several sophisticated end effectors have also been designed for this purpose. 
But for all of them to work the drone needs to identify the target object in front of it.
So the focus of this project is to identify an electrical panel door and its door handle and measure the distance of the drone from it.
This information will be later used (in a separate project) by the drone for controlling its position and the movement of its end effector to grab the door handle and pull open the door.

This project is only to about the image processing part to identify and electrical panel door. 
The camera to be used should be small and light, so that it can be put on a drone. So we selected the Intel Realsense Depth camera for this purpose.

# Requirements: 
* Algorithm should be able to run in real time on a laptop as well as on a sigle board computer (without any dependence on GPUs).
* Algorithm should be able to detect the electrical panel door and show the position of the door handle.
* The distance of the door handle from the camera should also be calculated continuously in real time.
* All software should be open source. 
* Overall setup should be battery operated and should be small and light enough to be mounted on a drone. 

# Current Framework: 
* Opencv libraries, Ubuntu 16.04. 
* Intel Realsense R200 Depth Camera.
* Odroid XU4 single board computer, Laptop computer.

#### Intel Realsense R200 Depth Camera and Odroid XU4:
![intel_realsense](images/intel_realsense_r200.png)
![Odroid_XU4](images/odroid_XU4.jpg)

#### Overall Setup mounted on the test Drone:
**[ Odroid is inside the white case ]**
![setup_on_drone_1](images/setup_on_drone_1.jpg)
![setup_on_drone_2](images/setup_on_drone_2.jpg)
![setup_on_drone_3](images/setup_on_drone_3.jpg)
![setup_on_drone_4](images/setup_on_drone_4.jpg)




# Algorithm Description: 
  
## Algorithm for detecting stairs moving down (Down-stairs): 
This algorithm only considers the first two steps of the stairs. This is done for two reasons.  
1. So that it can detect the stairs that have only two steps.  
2. As a person climbs down, the number of steps visible to him becomes fewer. And just before reaching the ground, only two  
or three steps might be visible.  

**So, why not make the algorithm try to detect only one step then?** 
Detection of a single step is also possible, but there would be a lot of false detections. There are several objects 
that resembles a stair with a single step, like a concrete beam lying on the ground or the edges of the sidewalk beside the streets, etc. Hence, considering only the first two steps of the stairs, seems to be the optimal choice. 

### Preprocessing of the images for down-stairs: 
To filter out the unwanted objects like walls or handrails, we consider only the lower central part of the image for our  
analysis (the interest region). This interest region is shown below (right image is **BGR image**, left is **Depth image**). 

![](images/down_stairs_interest_region.png) 

### Feature Extraction for â€œdown-stairsâ€: 
Multiple parallel scans of the points in the interest region are taken (the dots in the images). 
The depths of the scanned points are plotted against the y-coordinates of their corresponding pixels, which gives a plot like  
the following: 

![](images/down_stairs_depth_vs_y_coordinate.png) 

The plot shows a sudden change in depth of the scanned points at the locations corresponding to the edges of the stairs.  
These points adjacent to the edges are the feature points. 
Since only first two steps are considered, so the two points adjacent to the first step, and the two adjacent to the second,  
will comprise a set of features for a single scan in the interest region. The points A and B show the location of the first  
and the second edges of the stairs. 

An example of the four feature points of one particular scan is shown in the following figure. 

![](images/down_stairs_feature_points_location.png) 

P1 = Scanned Point just **below** the **first edge** location. 
P2 = Scanned Point just **above** the **first edge** location. 
P3 = Scanned Point just **below** the **second edge** location. 
P4 = Scanned Point just **above** the **second edge** location. 

### Parameterized model of â€œdown-stairsâ€: 
In practice, there might be some other objects in the scene that can also have edges, e.g. the edge of a shelf, chair, set of drawers, etc. So, to know that these features truly represent a â€œdown-stairâ€, we define a set of functions that describes the relationship between these features which constitutes a parameterized model of the â€œdown-stairsâ€. 

**1. FUNCTION_1:** 
P2.depth = ğœƒ00 + ğœƒ10 * P1.y + ğœƒ20 * P1.depth. 

Function of depth of point P2 in terms of the y-coordinate and depth of point P1. 

**2. FUNCTION_2:** 
P3.depth = ğœƒ01 + ğœƒ11 * P1.y + ğœƒ21 * P1.depth. 

Function of depth of point P3 in terms of the y-coordinate and depth of point P1. 

**3. FUNCTION_3:** 
P3.y = ğœƒ02 + ğœƒ12 * P1.x + ğœƒ22 * P1.y + ğœƒ32 * P1.depth. 

Function of the y-coordinate of point P3 in terms of the x-coordinate, y-coordinate, and depth of point P1. 

**4. FUNCTION_4:** 
AvD_P2_P3 = ğœƒ03 + ğœƒ13 * P2.x + ğœƒ23 * P2.y + ğœƒ33 * P1.depth. 

Average depth of all the points between P2 and P3 is represented by AvD_P2_P3.  
Function of the average depth of all the points between P2 and P3 in terms of the x-coordinate, y-coordinate, and depth of  
point P2. 

All the ğœƒ are parameters that are determined by linear regression over 53 different example images of the actual REAL and  
MODEL â€œdown-stairsâ€. 

**Parameters: MODEL â€œdown-stairsâ€:** 

ğœƒ00 = 164.2443 ; ğœƒ10 = -0.2036 ; ğœƒ20 = 1.0059 

ğœƒ01 = 184.0495 ; ğœƒ11 = 0.0413 ; ğœƒ21 = 0.9777 

ğœƒ02 = -47.0351 ; ğœƒ12 = 0.0163 ; ğœƒ22 = 0.7540 ; ğœƒ32 = 0.075 

ğœƒ03 = 13.7039 ; ğœƒ13 = -0.0537 ; ğœƒ23 = 0.082 ; ğœƒ33 = 0.9937 

**Parameters: REAL â€œdown-stairsâ€** 

ğœƒ00 = 252.523 ; ğœƒ10 = -0.3195 ; ğœƒ20 = 0.9829 

ğœƒ01 = 334.1068 ; ğœƒ11 = -0.0223 ; ğœƒ21 = 0.985 

ğœƒ02 = -85.0389 ; ğœƒ12 = -0.0056 ; ğœƒ22 = 0.7835 ; ğœƒ32 = 0.041 

ğœƒ03 = 40.4137 ; ğœƒ13 = -0.0064 ; ğœƒ23 = 0.1426 ; ğœƒ33 = 1.001 

### How the algorithm works: 
The interest region is first extracted from every frame of the BGR and depth video feed of the Kinect. This region is then  
scanned to search for feature points. If there are at least two locations along these scans, where the depth changes  
abruptly, then (assuming them to be potential stair edges) the points adjacent to these locations are extracted as the four  
feature points (P1, P2, P3, P4). The x and y coordinates and the depths of these points are saved for further analysis.  
Their values are then plugged into the functions of the parameterized model. Now, the algorithm already knows what the  
output values of these functions should be if the camera is really looking at the model â€œdown-stairsâ€. If we observe that  
the outputs of the functions are within some close acceptable thresholds of those values, then the algorithm declares that  
the â€œmodel down-stairsâ€ is detected. If there was some other object that the camera is looking at, then the functions of the  
parameterized model will never give proper values all at the same time. Once a stair is found, the edges are marked, and the  
distance of the edges from the camera is displayed, as shown in following figures. 

**Program modes:** 
The program can run in two modes â€“ the **MODEL stairs detection mode** and the **REAL stairs detection mode**. The selected  
mode is shown near the top right side of the display window. By default the program starts up in the MODEL stairs mode.  
Pressing the â€˜râ€™ key on the keyboard switches it into REAL stairs detection mode. Pressing â€˜mâ€™ switches the program back to  
MODEL stairs mode. The distance of the stairs from the camera is shown near the top left side of the display window. 

![](images/down_stairs_detected_model_stairs.png) 

![](images/down_stairs_detected_real_stairs.png) 

## Algorithm for detecting stairs moving up (Up-stairs): 

### Assumptions and Conventions: 
Here also the first two steps of the stairs are considered for the same reason. 

### Preprocessing of the images for: MODEL â€œup-stairsâ€: 
Interest region for filtering out the unwanted objects is shown in the following figure. 

![](images/up_stairs_interest_region.png) 

### Feature Extraction from the images for up-stairs: 
Multiple parallel scans of the points in the interest region are taken. The black and red dots in the depth and BGR images  
shows these scanned points. 
Plotting the depths of the scanned points against the y-coordinates of their corresponding pixels gives the following  
figure. 

![](images/up_stairs_depth_vs_y_coordinate.png) 

There is a change in the slope of the graph at the locations corresponding to the inner edges and also at their outer edges  
of the steps. At each of the inner edges the graph hits a local maxima, and at each outer edge, there is a local minima.  
These maxima and minima points will be the feature points. The points D and B show the maxima points and C and A show the minima points. 

An example of the four feature points of one scan is shown in the following figure. 

P1 = Scanned Point on the **inner edge** of the **first step** (first maxima). 
P2 = Scanned Point on the **outer edge** of the **first step** (first minima). 
P3 = Scanned Point on the **inner edge** at the **second step** (second maxima). 
P4 = Scanned Point on the **outer edge** of the **second step** (second minima). 

![](images/up_stairs_feature_points_location.png) 

### Parameterized model of up-stairs: 
To know that these features truly represent â€œup-stairsâ€, we define a set of functions that describes the  
relationship between these features. This constitutes a parameterized model of the Model â€œup-stairsâ€. 

**1. FUNCTION_1:** 
P2.depth = ğœ™00 + ğœ™10 * P1.y + ğœ™20 * P1.depth. 

Function of depth of point P2 in terms of the y-coordinate and depth of point P1. 

**2. FUNCTION_2:** 
P3.depth = ğœ™01 + ğœ™11 * P1.y + ğœ™21 * P1.depth. 

Function of the depth of point P3 in terms of the y-coordinate and depth of point P1.\ 

**3. FUNCTION_3:** 
P3.y = ğœ™02 + ğœ™12 * P1.x + ğœ™22 * P1.y + ğœ™32 * P1.depth. 

Function of the y-coordinate of point P3 in terms of the x-coordinate, y-coordinate, and depth of point P1. 

**4. FUNCTION_4:** 
AvD_P2_P3 = ğœ™03 + ğœ™13 * P2.x + ğœ™23 * P2.y + ğœ™33 * P1.depth 

Average depth of all the points between P2 and P3 is represented by AvD_P2_P3.  
Function of the average depth of all the points between P2 and P3 in terms of the x-coordinate, y-coordinate, and depth of  
point P2. 

**5. FUNCTION_5:** 
Slope_P1_P3 = (P1.depth â€“ P3.depth) / (P1.y â€“ P3.y) 

Slope of the line joining the points P1 and P3 is referred to as Slope_P1_P3. 

**6. FUNCTION_6:** 
Slope_P2_P4 = (P2.depth â€“ P4.depth) / (P2.y â€“ P4.y) 

Slope of the line joining the points P2 and P4 is referred to as Slope_P2_P4. 

All the ğœ™ are parameters that are determined by linear regression over 59 different example images of the actual REAL and  
MODEL â€œup-stairsâ€. 

**Parameters: MODEL â€œup-stairsâ€** 

ğœ™00 = -97.3592 ; ğœ™10 = 0.0585 ; ğœ™20 = 0.9768 

ğœ™01 = -36.7519 ; ğœ™11 = 0.0494 ; ğœ™21 = 1.0065 

ğœ™02 = -349.7489 ; ğœ™12 = -0.0112 ; ğœ™22 = 1.0507 ; ğœ™32 = 0.2898 

ğœ™03 = 28.0292 ; ğœ™13 = -0.0081 ; ğœ™23 = -0.0013 ; ğœ™33 = 1.0301 

**Parameters: REAL â€œup-stairsâ€** 

ğœ™00 = -220.2735 ; ğœ™10 = 0.0945 ; ğœ™20 = 1.0829 

ğœ™01 = -5.9768 ; ğœ™11 = 0.0802 ; ğœ™21 = 1.0612 

ğœ™02 = -351.6235 ; ğœ™12 = 0.0031 ; ğœ™22 = 0.9932 ; ğœ™32 = 0.1894 

ğœ™03 = 118.7745 ; ğœ™13 = -0.0120 ; ğœ™23 = 0.0253 ; ğœ™33 = 1.0189 

### How the algorithm works: 
The interest region is scanned to search for feature points. If there are at least two local minima and two local maxima  
points along these scans, then (assuming them to be potential stair edges) the points are extracted as the four feature  
points (P1, P2, P3, P4). As described in the previous sections, the x and y coordinates and the depths of these points are  
saved for further analysis. Their values are then plugged into the functions of the parameterized model. If we observe that  
the outputs of the functions are within some close acceptable thresholds of those values, then the algorithm declares that  
the Model â€œup-stairsâ€ is detected. If there is some other object that the camera is looking at, the functions of the  
parameterized model will never give proper values all at the same time. Once a stair is found, the edges are marked, and the  
distance of the edges from the camera is displayed, as shown in figure below. 

![](images/up_stairs_detected_model_stairs.png) 

![](images/up_stairs_detected_real_stairs.png) 

# Portable Hardware Setup for testing: 
Finally a portable wheeled platform is created for the entire setup. 
This was designed in **Solidworks**. 
The cad drawing and the image of the actual setup is shown below. 
The Solidworks files and images are also present in [solidworks_files](solidworks_files) directory.

**Setup for detecting MODEL stairs:** 

![](images/portable_setup_for_model_stairs_detection.png) 

**Setup for detecting REAL stairs:** 

![](images/portable_setup_for_real_stairs_detection.png) 

These are not two different setups. The upper part of the platform containing the kinect can be detached and mounted at the  
top of the lower section to configure it for REAL stairs. 
The kinect is also powered by a 12v battery to make the entire setup portable. 

# Results: 
The final working of the [detect_stairs](codes/detect_stairs.cpp) 
can be seen in the videos in the [test_videos](testing_videos) directory. 

The videos can also be viewed on **Youtube** using the following links: 

* [video of detection of MODEL DOWN stairs](https://www.youtube.com/watch?v=G00To1GtQtI&feature=youtu.be) 

* [video of detection of MODEL UP stairs](https://www.youtube.com/watch?v=KZ8WtoTPJZ0&feature=youtu.be) 

* [video of detection of REAL DOWN stairs](https://www.youtube.com/watch?v=ioiHB1MyEKs&feature=youtu.be) 

* [video of detection of REAL UP stairs](https://www.youtube.com/watch?v=NkCATsoHP4I&feature=youtu.be) 

Some snaps of the working system (for real stairs) can be seen in the following gifs.

**REAL stairs (DOWN):** 

![](testing_videos/vid_real_dn_stairs.gif) 

**REAL stairs (UP):** 

![](testing_videos/vid_real_up_stairs.gif) 

# Future Improvements:  
* Use of a smaller depth camera like the Intel Realsense. 
* Use of compact single board computer like the Odroid or Raspberri Pi or Nvidia Jetson etc. 
* Use of Deep Learning framework. 
* Integrate the camera to some kind of eyewear (like glasses), so that stairs are still visible if the user wears trousers. 

  

 